#!/usr/bin/env ansible-playbook
# (c) 2016 DataNexus Inc.  All Rights Reserved.
#
# overlay broker-to-broker, client-to-broker encryption over top of an existing kafka cluster
# AWS_PROFILE=datanexus ./PGPGdemo -e "cloud=aws region=us-east-1 application=kafka domain=development project=demo tenant=datanexus key_path=/Users/christopher/Documents/DataNexus/Demos/us-east-1  ansible_user=centos tenant_config_path=/Users/christopher/Documents/DataNexus/Platform/Source/datanexus source_home=/Users/christopher/Documents/DataNexus/Platform/Source"
---     
- name: KAFKA PRODUCER OVERLAY | discovering all {{ application }} nodes
  hosts: localhost
  tasks:
    - block:
      - ec2_remote_facts:
          region: "{{ region }}"
          filters:
            instance-state-name: running
            "tag:Application": "{{ application }}"
            "tag:Domain": "{{ domain }}"
            "tag:Project": "{{ project }}"
            "tag:Tenant": "{{ tenant }}"
        register: kafka_instances
        when:
          - cloud == 'aws'

      - name: KAFKA PRODUCER OVERLAY | building {{ application }} host group
        add_host: hostname="{{ item }}" groupname="{{ application }}" ansible_ssh_private_key_file="{{ key_path }}/{{ cloud }}-{{ region }}-{{ project }}-{{ application }}-{{ domain }}-private-key.pem"
        with_items: "{{ kafka_instances.instances|map(attribute='private_ip_address')|list }}"
        when:
          - kafka_instances.instances|length > 0
      when:
        - cloud == 'aws'

- name: KAFKA PRODUCER OVERLAY | discovering postgresql source instance
  hosts: localhost
  tasks:
    - block:
      - ec2_instance_facts:
          region: "{{ region }}"
          filters:
            instance-state-name: running
            "tag:Domain": "{{ domain }}"
            "tag:Application": postgresql
            "tag:Project": "{{ project }}"
            "tag:Tenant": "{{ tenant }}"
            "tag:Cluster": "{{ cluster | default ('a') }}"
#            "tag:Dataflow": "{{ dataflow | default ('none') }}"
        register: postgresql_instances
      
      - name: KAFKA PRODUCER OVERLAY | building postgresql source host group
        add_host: hostname="{{ item }}" groupname=postgresql_source ansible_ssh_private_key_file="{{ key_path }}/{{ cloud }}-{{ region }}-{{ project }}-postgresql-{{ domain }}-private-key.pem"
        with_items: "{{ postgresql_instances.instances | map(attribute='private_ip_address') | list }}"
        when:
          - postgresql_instances is defined
          - postgresql_instances.instances | length > 0
      when:
        - cloud == 'aws'
        
# there has to be a better way to gather the eth1 IP address; wtb cloud agnostic inventory system
- name: KAFKA PRODUCER OVERLAY | building demo table
  hosts: postgresql_source
  vars_files:
    - "{{ source_home }}/postgresql/vars/postgresql.yml"
  gather_facts: yes
  tasks:
    - command: "/usr/bin/psql -c 'DROP TABLE IF EXISTS t1;'"
      become: true
      become_user: "{{ postgresql_user }}"
    - command: "/usr/bin/psql -c 'CREATE TABLE IF NOT EXISTS t1 (id SERIAL, name VARCHAR);'"
      become: true
      become_user: "{{ postgresql_user }}"
    - command: /usr/bin/psql -c "INSERT INTO t1 (name) values ('a');"
      become: true
      become_user: "{{ postgresql_user }}"
    - command: /usr/bin/psql -c "INSERT INTO t1 (name) values ('b');"
      become: true
      become_user: "{{ postgresql_user }}"
    - command: /usr/bin/psql -c "INSERT INTO t1 (name) values ('c');"
      become: true
      become_user: "{{ postgresql_user }}"
    - command: /usr/bin/psql -c "INSERT INTO t1 (name) values ('d');"
      become: true
      become_user: "{{ postgresql_user }}"

- name: KAFKA PRODUCER OVERLAY | discovering postgresql sink instance
  hosts: localhost
  tasks:
    - block:
      - ec2_instance_facts:
          region: "{{ region }}"
          filters:
            instance-state-name: running
            "tag:Domain": "{{ domain }}"
            "tag:Application": postgresql
            "tag:Project": "{{ project }}"
            "tag:Tenant": "{{ tenant }}"
            "tag:Cluster": "{{ cluster | default ('b') }}"
#            "tag:Dataflow": "{{ dataflow | default ('none') }}"
        register: postgresql_instances
      
      - name: KAFKA PRODUCER OVERLAY | building postgresql sink host group
        add_host: hostname="{{ item }}" groupname=postgresql_sink ansible_ssh_private_key_file="{{ key_path }}/{{ cloud }}-{{ region }}-{{ project }}-postgresql-{{ domain }}-private-key.pem"
        with_items: "{{ postgresql_instances.instances | map(attribute='private_ip_address') | list }}"
        when:
          - postgresql_instances is defined
          - postgresql_instances.instances | length > 0
      when:
        - cloud == 'aws'

- name: KAFKA PRODUCER OVERLAY | registering sink interfaces
  hosts: postgresql_sink
  gather_facts: yes
  tasks:
    - setup:
                     
- name: KAFKA PRODUCER OVERLAY | discovering shaw
  hosts: localhost
  tasks:
    - block:
      - ec2_instance_facts:
          region: "{{ region }}"
          filters:
            instance-state-name: running
            "tag:Domain": "{{ domain }}"
            "tag:Project": "{{ project }}"
            "tag:Tenant": "{{ tenant }}"
            "tag:Application": shaw
        register: shaw_instances

      - name: KAFKA PRODUCER OVERLAY | building shaw host group
        add_host: hostname="{{ item }}" groupname=shaw ansible_ssh_private_key_file="{{ key_path }}/{{ cloud }}-{{ region }}-{{ project }}-shaw-{{ domain }}-private-key.pem"
        with_items: "{{ shaw_instances.instances | map(attribute='private_ip_address') | list }}"
        when:
          - shaw_instances is defined
          - shaw_instances.instances | length > 0
      when:
        - cloud == 'aws'

- name: KAFKA PRODUCER OVERLAY | shaw
  hosts: shaw
  gather_facts: yes
  tasks:
    - include_vars: "{{ source_home }}/postgresql/vars/postgresql.yml"
      when: source_home is defined

    - include_role:
        name: clients
        tasks_from: fetch-credentials
      vars:
        src_path: /etc/tls/datanexus/postgresql
        dst_path: "{{ key_path }}"
        subject: "{{ postgresql_user }}"
      with_items:
        - postgresql-{{ postgresql_user }}.crt
        - postgresql-{{ postgresql_user }}.key
    
- name: KAFKA PRODUCER OVERLAY | configuring {{ application }} certificates and stores
  hosts: "{{ application }}"
  vars_files:
      - "{{ source_home }}/kafka/vars/{{ application }}.yml"
      - "{{ source_home }}/postgresql/vars/postgresql.yml"
      - "{{ tenant_config_path }}/config/site.yml"
  gather_facts: true
  tasks:
    - set_fact: store_password=datanexus
    
    - block:
      - name: KAFKA PRODUCER OVERLAY | ensuring PostgreSQL certificate directory exists
        file:
          path: /etc/tls/datanexus/postgresql
          owner: "{{ kafka_user }}"
          group: "{{ kafka_user }}"
          state: directory
          mode: 0700
      
      - name: KAFKA PRODUCER OVERLAY | ensuring CA certificate directory exists
        file:
          path: /etc/tls/datanexus/ca
          owner: "{{ kafka_user }}"
          group: "{{ kafka_user }}"
          state: directory
          mode: 0700

      - name: KAFKA PRODUCER OVERLAY | distributing CA certificate key to all {{ application }} nodes
        copy:
          src: "{{ key_path }}/ca-cert.pem"
          dest: /etc/tls/datanexus/ca/ca-cert.pem
          owner: "{{ kafka_user }}"
          group: "{{ kafka_user }}"
          mode: 0600
          
      - name: KAFKA PRODUCER OVERLAY | distributing {{ postgresql_user }} key to all {{ application }} nodes
        copy:
          src:  "{{ key_path }}/postgresql-{{ postgresql_user }}.key"
          dest: /etc/tls/datanexus/postgresql/postgresql.key
          owner: "{{ kafka_user }}"
          group: "{{ kafka_user }}"
          mode: 0600

      - name: KAFKA PRODUCER OVERLAY | distributing {{ postgresql_user }} certificate to all {{ application }} nodes
        copy:
          src:  "{{ key_path }}/postgresql-{{ postgresql_user }}.crt"
          dest: /etc/tls/datanexus/postgresql/postgresql.crt
          owner: "{{ kafka_user }}"
          group: "{{ kafka_user }}"
          mode: 0600      
      
      - name: KAFKA PRODUCER OVERLAY | adding all kafka servers as bootstrap
        lineinfile:
          path: /etc/kafka/connect-distributed.properties
          regexp: '^bootstrap.servers=localhost:9092'
          line: "bootstrap.servers={{ hostvars[inventory_hostname].ansible_eth1.ipv4.address }}:9092"

      - name: KAFKA PRODUCER OVERLAY | adding ssl configuration to connect distributed properties
        blockinfile:
          path: /etc/kafka/connect-distributed.properties
          insertafter: EOF
          state: present
          block: |
            ssl.keystore.location = "/etc/tls/datanexus/kafka/kafka.server.keystore.jks"
            ssl.keystore.password = "{{ store_password }}"
            ssl.key.password = "{{ store_password }}"
          
      become: true
      
    - name: KAFKA PRODUCER OVERLAY | checking /etc/tls/datanexus/postgresql/truststore
      stat: path=/etc/tls/datanexus/postgresql/truststore
      become: true
      register: existing_store
      
    - name: KAFKA PRODUCER OVERLAY | removing existing trust store alias
      shell: "/bin/keytool -delete -alias postgreSQLCACert -keystore /etc/tls/datanexus/postgresql/truststore -storepass {{ store_password }}"
      become: true
      become_user: "{{ kafka_user }}"
      when: existing_store.stat.exists == True
      
    - name: KAFKA PRODUCER OVERLAY | building kafka postgresql connector trust store using root CA
      become: true
      become_user: "{{ kafka_user }}"
      shell: "/bin/keytool -import -alias postgreSQLCACert -file /etc/tls/datanexus/ca/ca-cert.pem -keystore /etc/tls/datanexus/postgresql/truststore -storepass {{ store_password }} -noprompt"

    - name: KAFKA PRODUCER OVERLAY | exporting postgresql certificates
      become: true
      become_user: "{{ kafka_user }}"
      shell: "/usr/bin/openssl pkcs12 -export -inkey /etc/tls/datanexus/postgresql/postgresql.key -in /etc/tls/datanexus/postgresql/postgresql.crt -name my-key -out /etc/tls/datanexus/postgresql/client.p12 -passout pass:{{ store_password }}"
  
    - name: KAFKA PRODUCER OVERLAY | building kafka postgresql connector key store
      become: true
      become_user: "{{ kafka_user }}"
      shell: "/bin/keytool -importkeystore -srckeystore /etc/tls/datanexus/postgresql/client.p12 -srcstoretype pkcs12 -destkeystore /etc/tls/datanexus/postgresql/keystore -deststoretype pkcs12 -storepass datanexus -noprompt -srcstorepass {{ store_password }}"
    
    - name: KAFKA PRODUCER OVERLAY | installing postgresql connector template
      template:
        src: postgresql-connector.sh.j2
        dest: /usr/local/bin/postgresql-connector.sh
        mode: 0755
        owner: "{{ kafka_user }}"
        group: "{{ kafka_group }}"
      become: true
    
    - name: KAFKA PRODUCER OVERLAY | starting distributed connector
      shell: /usr/local/bin/postgresql-connector.sh
      become: true
      become_user: "{{ kafka_user }}"
    
    - name: KAFKA PRODUCER OVERLAY | installing postgresql source template
      template:
        src: postgresql-source.sh.j2
        dest: /usr/local/bin/postgresql-source.sh
        mode: 0755
        owner: "{{ kafka_user }}"
        group: "{{ kafka_group }}"
      become: true
    
    - name: KAFKA PRODUCER OVERLAY | installing optional ssl configuration for consumers
      template:
        src: ssl.properties.j2
        dest: /etc/kafka/ssl.properties
        mode: 0755
        owner: "{{ kafka_user }}"
        group: "{{ kafka_group }}"
      become: true
      
    - name: KAFKA PRODUCER OVERLAY | pausing while connector spins up
      pause:
        seconds: 45

    - name: KAFKA PRODUCER OVERLAY | starting source
      shell: /usr/local/bin/postgresql-source.sh

    - name: KAFKA PRODUCER OVERLAY | installing postgresql sink template
      template:
        src: postgresql-sink.sh.j2
        dest: /usr/local/bin/postgresql-sink.sh
        mode: 0755
        owner: "{{ kafka_user }}"
        group: "{{ kafka_group }}"
      become: true
    
    - name: KAFKA PRODUCER OVERLAY | pausing while data flows
      pause:
        seconds: 5
          
    - name: KAFKA PRODUCER OVERLAY | starting sink
      shell: /usr/local/bin/postgresql-sink.sh
