#!/usr/bin/env ansible-playbook
# (c) 2016 DataNexus Inc.  All Rights Reserved.
#
# overlay broker-to-broker, client-to-broker encryption over top of an existing kafka cluster
# AWS_PROFILE=datanexus ./PGPGdemo -e "cloud=aws region=us-east-1 application=kafka domain=development project=demo tenant=datanexus key_path=/Users/christopher/Documents/DataNexus/Demos/us-east-1  ansible_user=centos tenant_config_path=/Users/christopher/Documents/DataNexus/Platform/Source/datanexus source_home=/Users/christopher/Documents/DataNexus/Platform/Source"
---     
- name: KAFKA PRODUCER OVERLAY | discovering all zookeeper nodes
  hosts: localhost
  tasks:
    - block:
      - ec2_remote_facts:
          region: "{{ region }}"
          filters:
            instance-state-name: running
            "tag:Application": zookeeper
            "tag:Domain": "{{ domain }}"
            "tag:Project": "{{ project }}"
            "tag:Tenant": "{{ tenant }}"
        register: zookeeper_instances
        when:
          - cloud == 'aws'

      - name: KAFKA PRODUCER OVERLAY | building zookeeper host group
        add_host: hostname="{{ item }}" groupname="zookeeper" ansible_ssh_private_key_file="{{ key_path }}/{{ cloud }}-{{ region }}-{{ project }}-zookeeper-{{ domain }}-private-key.pem"
        with_items: "{{ zookeeper_instances.instances | map(attribute='private_ip_address') | list }}"
        when:
          - zookeeper_instances.instances|length > 0
      when:
        - cloud == 'aws'

- name: KAFKA PRODUCER OVERLAY | discovering all cassandra nodes
  hosts: localhost
  tasks:
    - block:
      - ec2_remote_facts:
          region: "{{ region }}"
          filters:
            instance-state-name: running
            "tag:Application": cassandra
            "tag:Domain": "{{ domain }}"
            "tag:Project": "{{ project }}"
            "tag:Tenant": "{{ tenant }}"
            "tag:Cluster": "{{ cluster | default ('a') }}"
        register: cassandra_instances
        when:
          - cloud == 'aws'

      - name: KAFKA PRODUCER OVERLAY | building cassandra host group
        add_host: hostname="{{ item }}" groupname="cassandra" ansible_ssh_private_key_file="{{ key_path }}/{{ cloud }}-{{ region }}-{{ project }}-cassandra-{{ domain }}-private-key.pem"
        with_items: "{{ cassandra_instances.instances | map(attribute='private_ip_address') | list }}"
        when:
          - cassandra_instances.instances|length > 0
      when:
        - cloud == 'aws'

- name: KAFKA PRODUCER OVERLAY | discovering all postgresql instances
  hosts: localhost
  tasks:
    - block:
      - ec2_instance_facts:
          region: "{{ region }}"
          filters:
            instance-state-name: running
            "tag:Domain": "{{ domain }}"
            "tag:Application": postgresql
            "tag:Project": "{{ project }}"
            "tag:Tenant": "{{ tenant }}"
#            "tag:Dataflow": "{{ dataflow | default ('none') }}"
        register: postgresql_instances
      
      - name: KAFKA PRODUCER OVERLAY | building postgresql source host group
        add_host: hostname="{{ item }}" groupname=postgresql ansible_ssh_private_key_file="{{ key_path }}/{{ cloud }}-{{ region }}-{{ project }}-postgresql-{{ domain }}-private-key.pem"
        with_items: "{{ postgresql_instances.instances | map(attribute='private_ip_address') | list }}"
        when:
          - postgresql_instances is defined
          - postgresql_instances.instances | length > 0
      when:
        - cloud == 'aws'
        
- name: KAFKA PRODUCER OVERLAY | discovering postgresql source instance
  hosts: localhost
  tasks:
    - block:
      - ec2_instance_facts:
          region: "{{ region }}"
          filters:
            instance-state-name: running
            "tag:Domain": "{{ domain }}"
            "tag:Application": postgresql
            "tag:Project": "{{ project }}"
            "tag:Tenant": "{{ tenant }}"
            "tag:Cluster": "{{ cluster | default ('a') }}"
#            "tag:Dataflow": "{{ dataflow | default ('none') }}"
        register: postgresql_instances
      
      - name: KAFKA PRODUCER OVERLAY | building postgresql source host group
        add_host: hostname="{{ item }}" groupname=postgresql_source ansible_ssh_private_key_file="{{ key_path }}/{{ cloud }}-{{ region }}-{{ project }}-postgresql-{{ domain }}-private-key.pem"
        with_items: "{{ postgresql_instances.instances | map(attribute='private_ip_address') | list }}"
        when:
          - postgresql_instances is defined
          - postgresql_instances.instances | length > 0
      when:
        - cloud == 'aws'
        
- name: KAFKA PRODUCER OVERLAY | discovering postgresql sink instance
  hosts: localhost
  tasks:
    - block:
      - ec2_instance_facts:
          region: "{{ region }}"
          filters:
            instance-state-name: running
            "tag:Domain": "{{ domain }}"
            "tag:Application": postgresql
            "tag:Project": "{{ project }}"
            "tag:Tenant": "{{ tenant }}"
            "tag:Cluster": "{{ cluster | default ('b') }}"
#            "tag:Dataflow": "{{ dataflow | default ('none') }}"
        register: postgresql_instances
      
      - name: KAFKA PRODUCER OVERLAY | building postgresql sink host group
        add_host: hostname="{{ item }}" groupname=postgresql_sink ansible_ssh_private_key_file="{{ key_path }}/{{ cloud }}-{{ region }}-{{ project }}-postgresql-{{ domain }}-private-key.pem"
        with_items: "{{ postgresql_instances.instances | map(attribute='private_ip_address') | list }}"
        when:
          - postgresql_instances is defined
          - postgresql_instances.instances | length > 0
      when:
        - cloud == 'aws'

- name: KAFKA PRODUCER OVERLAY | registering postgresql sink interfaces and cleaning demo destination tables
  hosts: postgresql_sink
  vars_files:
    - "{{ source_home }}/postgresql/vars/postgresql.yml"
  gather_facts: yes
  tasks:
    - setup:
    - name: KAFKA PRODUCER OVERLAY | removing postgres_t1 table
      command: "/usr/bin/psql -c 'DROP TABLE IF EXISTS postgres_t1;'"
      become: true
      become_user: "{{ postgresql_user }}"

    - name: KAFKA PRODUCER OVERLAY | removing cassandra_t2 table
      command: "/usr/bin/psql -c 'DROP TABLE IF EXISTS cassandra_t2;'"
      become: true
      become_user: "{{ postgresql_user }}"
              
- name: KAFKA PRODUCER OVERLAY | discovering all {{ application }} nodes
  hosts: localhost
  tasks:
    - block:
      - ec2_remote_facts:
          region: "{{ region }}"
          filters:
            instance-state-name: running
            "tag:Application": "{{ application }}"
            "tag:Domain": "{{ domain }}"
            "tag:Project": "{{ project }}"
            "tag:Tenant": "{{ tenant }}"
        register: kafka_instances
        when:
          - cloud == 'aws'

      - name: KAFKA PRODUCER OVERLAY | building {{ application }} host group
        add_host: hostname="{{ item }}" groupname="{{ application }}" ansible_ssh_private_key_file="{{ key_path }}/{{ cloud }}-{{ region }}-{{ project }}-{{ application }}-{{ domain }}-private-key.pem"
        with_items: "{{ kafka_instances.instances|map(attribute='private_ip_address')|list }}"
        when:
          - kafka_instances.instances|length > 0
      when:
        - cloud == 'aws'

- name: KAFKA PRODUCER OVERLAY | removing any prior connector and topics
  hosts: "{{ application }}"
  vars_files:
      - "{{ source_home }}/kafka/vars/{{ application }}.yml"
  gather_facts: yes
  tasks:
    - block:
      - name: KAFKA PRODUCER OVERLAY | installing kafka topic list utility
        template:
          src: list-topics.sh.j2
          dest: /usr/local/bin/list-topics.sh
          mode: 0755
          owner: "{{ kafka_user }}"
          group: "{{ kafka_group }}"
        become: true
        
      # - name: KAFKA PRODUCER OVERLAY | installing kafka topic deletion utility
      #   template:
      #     src: delete-topics.sh.j2
      #     dest: /usr/local/bin/delete-topics.sh
      #     mode: 0755
      #     owner: "{{ kafka_user }}"
      #     group: "{{ kafka_group }}"
      #
      # - name: KAFKA PRODUCER OVERLAY | installing kafka connector stop utility
      #   template:
      #     src: stop-connector.sh.j2
      #     dest: /usr/local/bin/stop-connector.sh
      #     mode: 0755
      #     owner: "{{ kafka_user }}"
      #     group: "{{ kafka_group }}"
  
      # - name: KAFKA PRODUCER OVERLAY | stopping the connector on all nodes
      #   shell: /usr/local/bin/stop-connector.sh
      #
      # - name: KAFKA PRODUCER OVERLAY | removing specified topics on a single node
      #   shell: /usr/local/bin/delete-topics.sh postgres_t1 cassandra_t2
      #   run_once: true
      #
      # - name: KAFKA PRODUCER OVERLAY | restarting kafka (it's the only way to be sure)
      #   service:
      #     name: kafka
      #     state: restarted
      #
      become: true
      
- name: KAFKA PRODUCER OVERLAY | building {{ application }} interface list
  hosts: localhost
  tasks:
    - name: KAFKA PRODUCER OVERLAY | building {{ application }} public internal host group
      add_host: hostname="{{ hostvars[item].ansible_eth1.ipv4.address }}" groupname="{{ application }}_public" ansible_ssh_private_key_file="{{ key_path }}/{{ cloud }}-{{ region }}-{{ project }}-{{ application }}-{{ domain }}-private-key.pem"
      with_items: "{{ groups[application] }}"
        
# there has to be a better way to gather the eth1 IP address; wtb cloud agnostic inventory system
- name: KAFKA PRODUCER OVERLAY | tuning all postgresql instances
  hosts: postgresql
  vars_files:
    - "{{ source_home }}/postgresql/vars/postgresql.yml"
  gather_facts: yes
  tasks:
    - block:
      # this  and let PostgreSQL manage things
      # this also needs to go into /etc/sysctl.conf
      - name: KAFKA PRODUCER OVERLAY | turning off the linux memory autocommit
        command: /usr/sbin/sysctl -w vm.overcommit_memory=2
      # - name: KAFKA PRODUCER OVERLAY | configuring postgresql max connections
 #        lineinfile:
 #          path: "{{ postgresql_config_path }}/postgresql.conf"
 #          regexp: '^max_connections = 100'
 #          line: "max_connections = 100                   # (change requires restart)"
 #          backrefs: yes
 #        become_user: "{{ postgresql_user }}"
      #       - name: KAFKA PRODUCER OVERLAY | configuring postgresql shared buffers
      #         lineinfile:
      #           path: "{{ postgresql_config_path }}/postgresql.conf"
      #           regexp: '^shared_buffers = 128MB'
      #           line: "shared_buffers = 1536MB                # min 128kB"
      #           backrefs: yes
      #         become_user: "{{ postgresql_user }}"
      #       - name: KAFKA PRODUCER OVERLAY | configuring postgresql effective cache
      #         lineinfile:
      #           path: "{{ postgresql_config_path }}/postgresql.conf"
      #           regexp: '^#effective_cache_size = 4GB'
      #           line: "effective_cache_size = 4GB"
      #           backrefs: yes
      #         become_user: "{{ postgresql_user }}"
      #       - name: KAFKA PRODUCER OVERLAY | configuring postgresql maintenance worker memory
      #         lineinfile:
      #           path: "{{ postgresql_config_path }}/postgresql.conf"
      #           regexp: '^#maintenance_work_mem = 64MB'
      #           line: "maintenance_work_mem = 384MB           # min 1MB"
      #           backrefs: yes
      #         become_user: "{{ postgresql_user }}"
      #       - name: KAFKA PRODUCER OVERLAY | configuring postgresql maximum wal size
      #         lineinfile:
      #           path: "{{ postgresql_config_path }}/postgresql.conf"
      #           regexp: '^#max_wal_size = 1GB'
      #           line: "max_wal_size = 2GB"
      #           backrefs: yes
      #         become_user: "{{ postgresql_user }}"
      #       - name: KAFKA PRODUCER OVERLAY | configuring postgresql minimum wal size
      #         lineinfile:
      #           path: "{{ postgresql_config_path }}/postgresql.conf"
      #           regexp: '^#min_wal_size = 80MB'
      #           line: "min_wal_size = 1GB"
      #           backrefs: yes
      #         become_user: "{{ postgresql_user }}"
      #       - name: KAFKA PRODUCER OVERLAY | configuring postgresql checkpoint completion target
      #         lineinfile:
      #           path: "{{ postgresql_config_path }}/postgresql.conf"
      #           regexp: '^#checkpoint_completion_target = 0.5'
      #           line: "checkpoint_completion_target = 0.9     # checkpoint target duration, 0.0 - 1.0"
      #           backrefs: yes
      #         become_user: "{{ postgresql_user }}"
      #       - name: KAFKA PRODUCER OVERLAY | configuring postgresql wal buffers
      #         lineinfile:
      #           path: "{{ postgresql_config_path }}/postgresql.conf"
      #           regexp: '^#wal_buffers = -1'
      #           line: "wal_buffers = 16MB                     # min 32kB, -1 sets based on shared_buffers"
      #           backrefs: yes
      #         become_user: "{{ postgresql_user }}"
            # - name: KAFKA PRODUCER OVERLAY | configuring postgresql default statistics target
      #         lineinfile:
      #           path: "{{ postgresql_config_path }}/postgresql.conf"
      #           regexp: '^#default_statistics_target = 100'
      #           line: "default_statistics_target = 100         # range 1-10000"
      #           backrefs: yes
      #         become_user: "{{ postgresql_user }}"
            # - name: KAFKA PRODUCER OVERLAY | configuring postgresql random page cost
           #    lineinfile:
           #      path: "{{ postgresql_config_path }}/postgresql.conf"
           #      regexp: '^#random_page_cost = 4.0'
           #      line: "random_page_cost = 4.0                  # same scale as above"
           #      backrefs: yes
           #    become_user: "{{ postgresql_user }}"
           #  - name: KAFKA PRODUCER OVERLAY | configuring postgresql effective io concurrency
           #    lineinfile:
           #      path: "{{ postgresql_config_path }}/postgresql.conf"
           #      regexp: '^#effective_io_concurrency = 1'
           #      line: "effective_io_concurrency = 2           # 1-1000; 0 disables prefetching"
           #      backrefs: yes
           #    become_user: "{{ postgresql_user }}"
           #  - name: KAFKA PRODUCER OVERLAY | configuring postgresql maximum worker processes
           #    lineinfile:
           #      path: "{{ postgresql_config_path }}/postgresql.conf"
           #      regexp: '^#max_worker_processes = 8'
           #      line: "max_worker_processes = 2               # (change requires restart)"
           #      backrefs: yes
           #    become_user: "{{ postgresql_user }}"
           #  - name: KAFKA PRODUCER OVERLAY | configuring postgresql maximum parallel workers per gather
           #    lineinfile:
           #      path: "{{ postgresql_config_path }}/postgresql.conf"
           #      regexp: '^#max_parallel_workers_per_gather = 0'
           #      line: "max_parallel_workers_per_gather = 1    # taken from max_worker_processes"
           #      backrefs: yes
           #    become_user: "{{ postgresql_user }}"
           #  - name: KAFKA PRODUCER OVERLAY | configuring postgresql work memory
           #    lineinfile:
           #      path: "{{ postgresql_config_path }}/postgresql.conf"
           #      regexp: '^#work_mem = 4MB'
           #      line: "work_mem = 4MB                         # min 64kB"
           #      backrefs: yes
           #    become_user: "{{ postgresql_user }}"
      - name: KAFKA PRODUCER OVERLAY | restarting postgresql
        service:
          name: "postgresql-{{ postgresql_major_version }}.{{ postgresql_minor_version }}"
          state: restarted
      become: true
      
# gather_facts or setup: will mine the system for eth1, but we need to do some prep work anyway
- name: KAFKA PRODUCER OVERLAY | discovering cassandra interfaces
  hosts: cassandra
  vars_files:
    - "{{ source_home }}/cassandra/vars/cassandra.yml"
  gather_facts: yes
  tasks:
    # because of the clustered nature of cassandra, we only need to run all commands on a single host
    - name: KAFKA PRODUCER OVERLAY | creating demo keyspace
      command: "/opt/apache-cassandra/bin/cqlsh -u {{ cassandra_user }} -p {{ cassandra_user }} {{ hostvars[inventory_hostname].ansible_eth1.ipv4.address }} -e \"CREATE KEYSPACE IF NOT EXISTS demo WITH REPLICATION = {'class' : 'SimpleStrategy', 'replication_factor' : 1};\""
      run_once: true
    
    - name: KAFKA PRODUCER OVERLAY | creating source table
      command: "/opt/apache-cassandra/bin/cqlsh -u {{ cassandra_user }} -p {{ cassandra_user }} {{ hostvars[inventory_hostname].ansible_eth1.ipv4.address }} -k demo -e 'CREATE TABLE IF NOT EXISTS t2 (id INT, created TIMEUUID, product TEXT, qty INT, price FLOAT, PRIMARY KEY (id, created)) WITH CLUSTERING ORDER BY (created ASC);'"
      run_once: true
    
    - name: KAFKA PRODUCER OVERLAY | truncating t2
      command: "/opt/apache-cassandra/bin/cqlsh -u {{ cassandra_user }} -p {{ cassandra_user }} {{ hostvars[inventory_hostname].ansible_eth1.ipv4.address }} -k demo -e \"TRUNCATE t2;\""
      run_once: true
        
    - name: KAFKA PRODUCER OVERLAY | creating demo destination table
      command: "/opt/apache-cassandra/bin/cqlsh -u {{ cassandra_user }} -p {{ cassandra_user }} {{ hostvars[inventory_hostname].ansible_eth1.ipv4.address }} -k demo -e 'CREATE TABLE IF NOT EXISTS cassandra_t1 (id INT, created TIMESTAMP, firstname VARCHAR, middlename VARCHAR, lastname VARCHAR, street VARCHAR, city VARCHAR, state VARCHAR, country VARCHAR, age VARCHAR, email VARCHAR, preferences VARCHAR, PRIMARY KEY(id, created));'"
      run_once: true
    
    - name: KAFKA PRODUCER OVERLAY | truncating cassandra_t1
      command: "/opt/apache-cassandra/bin/cqlsh -u {{ cassandra_user }} -p {{ cassandra_user }} {{ hostvars[inventory_hostname].ansible_eth1.ipv4.address }} -k demo -e \"TRUNCATE cassandra_t1;\""
      run_once: true
    
    - name: KAFKA PRODUCER OVERLAY | pausing to test timestamp differential
      pause:
        seconds: 1
        
    - name: KAFKA PRODUCER OVERLAY | inserting first data row
      command: "/opt/apache-cassandra/bin/cqlsh -u {{ cassandra_user }} -p {{ cassandra_user }} {{ hostvars[inventory_hostname].ansible_eth1.ipv4.address }} -k demo -e \"INSERT INTO t2 (id, created, product, qty, price) VALUES (1, now(), 'OP-DAX-P-20150201-95.7', 100, 94.2);\""
      run_once: true
    
    - name: KAFKA PRODUCER OVERLAY | pausing to test timestamp differential
      pause:
        seconds: 1
        
    - name: KAFKA PRODUCER OVERLAY | inserting second data row
      command: "/opt/apache-cassandra/bin/cqlsh -u {{ cassandra_user }} -p {{ cassandra_user }} {{ hostvars[inventory_hostname].ansible_eth1.ipv4.address }} -k demo -e \"INSERT INTO t2 (id, created, product, qty, price) VALUES (2, now(), 'OP-DAX-C-20150201-100', 100, 99.5);\""
      run_once: true
    
    - name: KAFKA PRODUCER OVERLAY | pausing to test timestamp differential
      pause:
        seconds: 1
        
    - name: KAFKA PRODUCER OVERLAY | inserting third data row
      command: "/opt/apache-cassandra/bin/cqlsh -u {{ cassandra_user }} -p {{ cassandra_user }} {{ hostvars[inventory_hostname].ansible_eth1.ipv4.address }} -k demo -e \"INSERT INTO t2 (id, created, product, qty, price) VALUES (3, now(), 'FU-KOSPI-C-20150201-100', 200, 150);\""
      run_once: true
    
    - name: KAFKA PRODUCER OVERLAY | pausing to test timestamp differential
      pause:
        seconds: 1
        
    - name: KAFKA PRODUCER OVERLAY | inserting fourth data row
      command: "/opt/apache-cassandra/bin/cqlsh -u {{ cassandra_user }} -p {{ cassandra_user }} {{ hostvars[inventory_hostname].ansible_eth1.ipv4.address }} -k demo -e \"INSERT INTO t2 (id, created, product, qty, price) VALUES (4, now(), 'CE-DATANEXUS-C-20150201-100', 500, 10000);\""
      run_once: true
    
    - name: KAFKA PRODUCER OVERLAY | pausing to test timestamp differential
      pause:
        seconds: 1
        
    - name: KAFKA PRODUCER OVERLAY | inserting fifth data row
      command: "/opt/apache-cassandra/bin/cqlsh -u {{ cassandra_user }} -p {{ cassandra_user }} {{ hostvars[inventory_hostname].ansible_eth1.ipv4.address }} -k demo -e \"INSERT INTO t2 (id, created, product, qty, price) VALUES (5, now(), 'CE-PLET-C-20181225-99.1', 200, 76.8);\""
      run_once: true
      
- name: KAFKA PRODUCER OVERLAY | building {{ application }} interface list
  hosts: localhost
  tasks:
    - name: KAFKA PRODUCER OVERLAY | building cassandra public internal host group
      add_host: hostname="{{ hostvars[item].ansible_eth1.ipv4.address }}" groupname="cassandra_public" ansible_ssh_private_key_file="{{ key_path }}/{{ cloud }}-{{ region }}-{{ project }}-cassandra-{{ domain }}-private-key.pem"
      with_items: "{{ groups['cassandra'] }}"
      run_once: true

# there has to be a better way to gather the eth1 IP address; wtb cloud agnostic inventory system
- name: KAFKA PRODUCER OVERLAY | building and populating demo table
  hosts: postgresql_source
  vars_files:
    - "{{ source_home }}/postgresql/vars/postgresql.yml"
  gather_facts: yes
  tasks:
    - block:
      # note that source tables *MUST* be created before workers are spun up
      - command: /usr/bin/psql -c 'CREATE TABLE IF NOT EXISTS t1 (id SERIAL, created TIMESTAMP NOT NULL, firstname VARCHAR, middlename VARCHAR, lastname VARCHAR, street VARCHAR, city VARCHAR, state VARCHAR, country VARCHAR, age VARCHAR, email VARCHAR, preferences VARCHAR);'
      
      - command: /usr/bin/psql -c 'DELETE FROM t1;'
  
      - name: KAFKA PRODUCER OVERLAY | inserting postgresql from 1 to 500,000 rows
        command: /usr/bin/psql -c "INSERT INTO t1 VALUES (generate_series(1,500000), now(), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text));"
      become: true
      become_user: "{{ postgresql_user }}" 
                     
- name: KAFKA PRODUCER OVERLAY | discovering shaw
  hosts: localhost
  tasks:
    - block:
      - ec2_instance_facts:
          region: "{{ region }}"
          filters:
            instance-state-name: running
            "tag:Domain": "{{ domain }}"
            "tag:Project": "{{ project }}"
            "tag:Tenant": "{{ tenant }}"
            "tag:Application": shaw
        register: shaw_instances

      - name: KAFKA PRODUCER OVERLAY | building shaw host group
        add_host: hostname="{{ item }}" groupname=shaw ansible_ssh_private_key_file="{{ key_path }}/{{ cloud }}-{{ region }}-{{ project }}-shaw-{{ domain }}-private-key.pem"
        with_items: "{{ shaw_instances.instances | map(attribute='private_ip_address') | list }}"
        when:
          - shaw_instances is defined
          - shaw_instances.instances | length > 0
      when:
        - cloud == 'aws'

- name: KAFKA PRODUCER OVERLAY | retrieving postgresql credentials from shaw
  hosts: shaw
  gather_facts: yes
  tasks:
    - include_vars: "{{ source_home }}/postgresql/vars/postgresql.yml"
      when: source_home is defined

    - include_role:
        name: clients
        tasks_from: fetch-credentials
      vars:
        src_path: /etc/tls/datanexus/postgresql
        dst_path: "{{ key_path }}"
        subject: "{{ postgresql_user }}"
      with_items:
        - postgresql-{{ postgresql_user }}.crt
        - postgresql-{{ postgresql_user }}.key
    
- name: KAFKA PRODUCER OVERLAY | configuring {{ application }} certificates and stores
  hosts: "{{ application }}"
  vars_files:
      - "{{ source_home }}/kafka/vars/{{ application }}.yml"
      - "{{ source_home }}/postgresql/vars/postgresql.yml"
      - "{{ tenant_config_path }}/config/site.yml"
  gather_facts: true
  tasks:
    - set_fact: store_password=datanexus
    
    - block:
      - name: KAFKA PRODUCER OVERLAY | ensuring PostgreSQL certificate directory exists
        file:
          path: /etc/tls/datanexus/postgresql
          owner: "{{ kafka_user }}"
          group: "{{ kafka_user }}"
          state: directory
          mode: 0700
      
      - name: KAFKA PRODUCER OVERLAY | ensuring CA certificate directory exists
        file:
          path: /etc/tls/datanexus/ca
          owner: "{{ kafka_user }}"
          group: "{{ kafka_user }}"
          state: directory
          mode: 0700

      - name: KAFKA PRODUCER OVERLAY | distributing CA certificate key to all {{ application }} nodes
        copy:
          src: "{{ key_path }}/ca-cert.pem"
          dest: /etc/tls/datanexus/ca/ca-cert.pem
          owner: "{{ kafka_user }}"
          group: "{{ kafka_user }}"
          mode: 0600
          
      - name: KAFKA PRODUCER OVERLAY | distributing {{ postgresql_user }} key to all {{ application }} nodes
        copy:
          src:  "{{ key_path }}/postgresql-{{ postgresql_user }}.key"
          dest: /etc/tls/datanexus/postgresql/postgresql.key
          owner: "{{ kafka_user }}"
          group: "{{ kafka_user }}"
          mode: 0600

      - name: KAFKA PRODUCER OVERLAY | distributing {{ postgresql_user }} certificate to all {{ application }} nodes
        copy:
          src: "{{ key_path }}/postgresql-{{ postgresql_user }}.crt"
          dest: /etc/tls/datanexus/postgresql/postgresql.crt
          owner: "{{ kafka_user }}"
          group: "{{ kafka_user }}"
          mode: 0600
    
      - name: KAFKA PRODUCER OVERLAY | adding all kafka servers as bootstrap for distibuted connectors
        lineinfile:
          path: /etc/kafka/connect-distributed.properties
          regexp: '^bootstrap.servers=localhost:9092'
          line: "bootstrap.servers={{ groups['kafka_public'] | join(':9092,') }}:9092"
          backrefs: yes
      
      - name: KAFKA PRODUCER OVERLAY | adding all kafka servers as bootstrap for distributed avro
        lineinfile:
          path: /etc/schema-registry/connect-avro-distributed.properties
          regexp: '^bootstrap.servers=localhost:9092'
          line: "bootstrap.servers={{ groups['kafka_public'] | join(':9092,') }}:9092"
          backrefs: yes
          
      # based 546byte message size
      # - name: KAFKA PRODUCER OVERLAY | tuning bytes sent
     #    lineinfile:
     #      state: present
     #      path: /etc/kafka/connect-distributed.properties
     #      regexp: '^consumer.fetch.min.bytes=550000'
     #      line: 'consumer.fetch.min.bytes=550000'
          
      # - name: KAFKA PRODUCER OVERLAY | tuning wait interval
 #        lineinfile:
 #          state: present
 #          path: /etc/kafka/connect-distributed.properties
 #          regexp: '^consumer.fetch.wait.max.ms=1000'
 #          line: 'consumer.fetch.wait.max.ms=1000'
      
      # - name: KAFKA PRODUCER OVERLAY | tuning wait interval for avro distributed connector
     #    lineinfile:
     #      state: present
     #      path: /etc/schema-registry/connect-avro-distributed.properties
     #      regexp: '^consumer.fetch.wait.max.ms=1000'
     #      line: 'consumer.fetch.wait.max.ms=1000'
      
      - name: KAFKA PRODUCER OVERLAY | updating plugin path for distributed connector
        lineinfile:
          state: present
          path: /etc/kafka/connect-distributed.properties
          regexp: '^#plugin.path='
          line: 'plugin.path=/usr/local/share/kafka/plugins'
          backrefs: yes
          
      - name: KAFKA PRODUCER OVERLAY | creating directory for 3rd party plugins
        file:
          path: /usr/local/share/kafka/plugins
          owner: "{{ kafka_user }}"
          group: "{{ kafka_user }}"
          state: directory
          mode: 0755
          
      - name: KAFKA PRODUCER OVERLAY | updating plugin path for distributed avro
        lineinfile:
          state: present
          path: /etc/schema-registry/connect-avro-distributed.properties
          regexp: '^#plugin.path='
          line: 'plugin.path=/usr/local/share/kafka/plugins'
          backrefs: yes
          
      - name: KAFKA PRODUCER OVERLAY | distributing cassandra plugin to all {{ application }} nodes
        unarchive:
          src: https://github.com/Landoop/stream-reactor/releases/download/0.4.0/kafka-connect-cassandra-0.4.0-0.11.0.1-all.tar.gz
          dest: /usr/local/share/kafka/plugins/
          owner: "{{ kafka_user }}"
          group: "{{ kafka_user }}"
          remote_src: yes
        
      - name: KAFKA PRODUCER OVERLAY | adding ssl configuration to connect distributed properties
        blockinfile:
          path: /etc/kafka/connect-distributed.properties
          insertafter: EOF
          state: present
          block: |
            ssl.keystore.location = "/etc/tls/datanexus/kafka/kafka.server.keystore.jks"
            ssl.keystore.password = "{{ store_password }}"
            ssl.key.password = "{{ store_password }}"
      
      - name: KAFKA PRODUCER OVERLAY | adding ssl configuration to avro connect distributed properties
        blockinfile:
          path: /etc/schema-registry/connect-avro-distributed.properties
          insertafter: EOF
          state: present
          block: |
            ssl.keystore.location = "/etc/tls/datanexus/kafka/kafka.server.keystore.jks"
            ssl.keystore.password = "{{ store_password }}"
            ssl.key.password = "{{ store_password }}"
      become: true
        
    - name: KAFKA PRODUCER OVERLAY | checking /etc/tls/datanexus/postgresql/truststore
      stat: path=/etc/tls/datanexus/postgresql/truststore
      become: true
      register: existing_store
      
    - name: KAFKA PRODUCER OVERLAY | removing existing trust store alias
      shell: "/bin/keytool -delete -alias postgreSQLCACert -keystore /etc/tls/datanexus/postgresql/truststore -storepass {{ store_password }}"
      become: true
      become_user: "{{ kafka_user }}"
      when: existing_store.stat.exists == True
      
    - name: KAFKA PRODUCER OVERLAY | building kafka postgresql connector trust store using root CA
      become: true
      become_user: "{{ kafka_user }}"
      shell: "/bin/keytool -import -alias postgreSQLCACert -file /etc/tls/datanexus/ca/ca-cert.pem -keystore /etc/tls/datanexus/postgresql/truststore -storepass {{ store_password }} -noprompt"

    - name: KAFKA PRODUCER OVERLAY | exporting postgresql certificates
      become: true
      become_user: "{{ kafka_user }}"
      shell: "/usr/bin/openssl pkcs12 -export -inkey /etc/tls/datanexus/postgresql/postgresql.key -in /etc/tls/datanexus/postgresql/postgresql.crt -name my-key -out /etc/tls/datanexus/postgresql/client.p12 -passout pass:{{ store_password }}"
  
    - name: KAFKA PRODUCER OVERLAY | building kafka postgresql connector key store
      become: true
      become_user: "{{ kafka_user }}"
      shell: "/bin/keytool -importkeystore -srckeystore /etc/tls/datanexus/postgresql/client.p12 -srcstoretype pkcs12 -destkeystore /etc/tls/datanexus/postgresql/keystore -deststoretype pkcs12 -storepass datanexus -noprompt -srcstorepass {{ store_password }}"    
    
    - name: KAFKA PRODUCER OVERLAY | installing kafka topic describe utility
      template:
        src: describe-topic.sh.j2
        dest: /usr/local/bin/describe-topic.sh
        mode: 0755
        owner: "{{ kafka_user }}"
        group: "{{ kafka_group }}"
      become: true
      
    - name: KAFKA PRODUCER OVERLAY | installing kafka connector topics create utility
      template:
        src: create-connector-topics.sh.j2
        dest: /usr/local/bin/create-connector-topics.sh
        mode: 0755
        owner: "{{ kafka_user }}"
        group: "{{ kafka_group }}"
      become: true
    
    - name: KAFKA PRODUCER OVERLAY | installing kafka data topic create utility
      template:
        src: create-data-topic.sh.j2
        dest: /usr/local/bin/create-data-topic.sh
        mode: 0755
        owner: "{{ kafka_user }}"
        group: "{{ kafka_group }}"
      become: true
      
    - name: KAFKA PRODUCER OVERLAY | installing kafka connector list utility
      template:
        src: list-connectors.sh.j2
        dest: /usr/local/bin/list-connectors.sh
        mode: 0755
        owner: "{{ kafka_user }}"
        group: "{{ kafka_group }}"
      become: true
      
    - name: KAFKA PRODUCER OVERLAY | installing kafka topic display utility
      template:
        src: cat-topic.sh.j2
        dest: /usr/local/bin/cat-topic.sh
        mode: 0755
        owner: "{{ kafka_user }}"
        group: "{{ kafka_group }}"
      become: true
    
    - name: KAFKA PRODUCER OVERLAY | installing kafka connector deletion utility
      template:
        src: delete-connector.sh.j2
        dest: /usr/local/bin/delete-connector.sh
        mode: 0755
        owner: "{{ kafka_user }}"
        group: "{{ kafka_group }}"
      become: true
    
    - name: KAFKA PRODUCER OVERLAY | installing kafka consumer group describe utility
      template:
        src: describe-group.sh.j2
        dest: /usr/local/bin/describe-group.sh
        mode: 0755
        owner: "{{ kafka_user }}"
        group: "{{ kafka_group }}"
      become: true
      
    - name: KAFKA PRODUCER OVERLAY | installing distributed connector start template
      template:
        src: distributed-connector.sh.j2
        dest: /usr/local/bin/distributed-connector.sh
        mode: 0755
        owner: "{{ kafka_user }}"
        group: "{{ kafka_group }}"
      become: true
    
    - name: KAFKA PRODUCER OVERLAY | installing avro distributed connector start template
      template:
        src: avro-distributed-connector.sh.j2
        dest: /usr/local/bin/avro-distributed-connector.sh
        mode: 0755
        owner: "{{ kafka_user }}"
        group: "{{ kafka_group }}"
      become: true
    
    - name: KAFKA PRODUCER OVERLAY | installing postgresql source template
      template:
        src: postgresql-source.sh.j2
        dest: /usr/local/bin/postgresql-source.sh
        mode: 0755
        owner: "{{ kafka_user }}"
        group: "{{ kafka_group }}"
      become: true
    
    - name: KAFKA PRODUCER OVERLAY | installing cassandra source template
      template:
        src: cassandra-source.sh.j2
        dest: /usr/local/bin/cassandra-source.sh
        mode: 0755
        owner: "{{ kafka_user }}"
        group: "{{ kafka_group }}"
      become: true
    
    - name: KAFKA PRODUCER OVERLAY | installing optional ssl configuration for CLI consumers
      template:
        src: ssl.properties.j2
        dest: /etc/kafka/ssl.properties
        mode: 0755
        owner: "{{ kafka_user }}"
        group: "{{ kafka_group }}"
      become: true
    
    - name: KAFKA PRODUCER OVERLAY | creating connector topics
      shell: /usr/local/bin/create-connector-topics.sh
      become: true
      become_user: "{{ kafka_user }}"
      run_once: true
      
    - name: KAFKA PRODUCER OVERLAY | creating topic postgres_t1 across 60 partitions with replication factor of 1
      shell: /usr/local/bin/create-data-topic.sh postgres_t1 60 1
      become: true
      become_user: "{{ kafka_user }}"
      run_once: true
    
    - name: KAFKA PRODUCER OVERLAY | creating topic cassandra_t2 across 3 partitions with replication factor of 1
      shell: /usr/local/bin/create-data-topic.sh cassandra_t2 3 1
      become: true
      become_user: "{{ kafka_user }}"
      run_once: true
      
    # enable this to use non-avro connector encoding
    - name: KAFKA PRODUCER OVERLAY | starting distributed connector
      shell: /usr/local/bin/distributed-connector.sh
      become: true
      become_user: "{{ kafka_user }}"
    
    # enable this to use avro connector encoding
    # - name: KAFKA PRODUCER OVERLAY | starting avro distributed connector
#       shell: /usr/local/bin/avro-distributed-connector.sh
#       become: true
#       become_user: "{{ kafka_user }}"
          
    - name: KAFKA PRODUCER OVERLAY | pausing while connector spins up
      pause:
        seconds: 30

    # does this need to happen on every node or just one?
    - name: KAFKA PRODUCER OVERLAY | starting postresql source worker
      shell: /usr/local/bin/postgresql-source.sh
    
    - name: KAFKA PRODUCER OVERLAY | starting cassandra source worker
      shell: /usr/local/bin/cassandra-source.sh

    - name: KAFKA PRODUCER OVERLAY | pausing while source workers register with connector
      pause:
        seconds: 20
        
    - name: KAFKA PRODUCER OVERLAY | installing postgresql sink template
      template:
        src: postgresql-sink.sh.j2
        dest: /usr/local/bin/postgresql-sink.sh
        mode: 0755
        owner: "{{ kafka_user }}"
        group: "{{ kafka_group }}"
      become: true
    
    - name: KAFKA PRODUCER OVERLAY | starting postgresql sink worker
      shell: /usr/local/bin/postgresql-sink.sh
      
    - name: KAFKA PRODUCER OVERLAY | installing cassandra sink template
      template:
        src: cassandra-sink.sh.j2
        dest: /usr/local/bin/cassandra-sink.sh
        mode: 0755
        owner: "{{ kafka_user }}"
        group: "{{ kafka_group }}"
      become: true
    
    - name: KAFKA PRODUCER OVERLAY | starting cassandra sink worker
      shell: /usr/local/bin/cassandra-sink.sh

    - name: KAFKA PRODUCER OVERLAY | pausing while sink workers register with connector
      pause:
        seconds: 20
        
- name: KAFKA PRODUCER OVERLAY | inserting additional postgresql rows
  hosts: postgresql_source
  vars_files:
    - "{{ source_home }}/postgresql/vars/postgresql.yml"
  gather_facts: yes
  tasks:
    - block:
      - name: KAFKA PRODUCER OVERLAY | pausing before inserting from 500,001 to 1,000,000 rows
        pause:
          seconds: 40
      - command: /usr/bin/psql -c "INSERT INTO t1 VALUES (generate_series(500001,1000000), now(), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text));"
      
      become: true
      become_user: "{{ postgresql_user }}"

    - name: KAFKA PRODUCER OVERLAY | pausing before inserting from 1,000,001 to 1,500,000 rows
      pause:
        seconds: 40
    - command: /usr/bin/psql -c "INSERT INTO t1 VALUES (generate_series(1000001,1500000), now(), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text));"
      become: true
      become_user: "{{ postgresql_user }}"

    - name: KAFKA PRODUCER OVERLAY | pausing before inserting from 1,500,001 to 2,000,000 rows
      pause:
        seconds: 40
    - command: /usr/bin/psql -c "INSERT INTO t1 VALUES (generate_series(1500001,2000000), now(), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text));"
      become: true
      become_user: "{{ postgresql_user }}"

    - name: KAFKA PRODUCER OVERLAY | pausing before inserting from 2,000,001 to 2,500,000 rows
      pause:
        seconds: 40
    - command: /usr/bin/psql -c "INSERT INTO t1 VALUES (generate_series(2000001,2500000), now(), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text));"
      become: true
      become_user: "{{ postgresql_user }}"

    - name: KAFKA PRODUCER OVERLAY | pausing before inserting from 2,500,001 to 3,000,000 rows
      pause:
        seconds: 40
    - command: /usr/bin/psql -c "INSERT INTO t1 VALUES (generate_series(2500001,3000000), now(), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text));"
      become: true
      become_user: "{{ postgresql_user }}"

    - name: KAFKA PRODUCER OVERLAY | pausing before inserting from 3,000,001 to 3,500,000 rows
      pause:
        seconds: 40
    - command: /usr/bin/psql -c "INSERT INTO t1 VALUES (generate_series(3000001,3500000), now(), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text));"
      become: true
      become_user: "{{ postgresql_user }}"

    # - name: KAFKA PRODUCER OVERLAY | pausing before inserting from 3,500,001 to 4,000,000 rows
    #   pause:
    #     seconds: 40
    # - command: /usr/bin/psql -c "INSERT INTO t1 VALUES (generate_series(3500001,4000000), now(), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text));"
    #   become: true
    #   become_user: "{{ postgresql_user }}"
    #
    # - name: KAFKA PRODUCER OVERLAY | pausing before inserting from 4,000,001 to 4,500,000 rows
    #   pause:
    #     seconds: 40
    # - command: /usr/bin/psql -c "INSERT INTO t1 VALUES (generate_series(4000001,4500000), now(), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text));"
    #   become: true
    #   become_user: "{{ postgresql_user }}"
    #
    # - name: KAFKA PRODUCER OVERLAY | pausing before inserting 500K rows 4,500,001 to 5,000,000
    #   pause:
    #     seconds: 40
    # - command: /usr/bin/psql -c "INSERT INTO t1 VALUES (generate_series(4500001,5000000), now(), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text));"
    #   become: true
    #   become_user: "{{ postgresql_user }}"
    #
    # - name: KAFKA PRODUCER OVERLAY | pausing before inserting from 5,000,001 to 5,500,000 rows
    #   pause:
    #     seconds: 40
    # - command: /usr/bin/psql -c "INSERT INTO t1 VALUES (generate_series(5000001,5500000), now(), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text));"
    #   become: true
    #   become_user: "{{ postgresql_user }}"
    #
    # - name: KAFKA PRODUCER OVERLAY | pausing before inserting from 5,500,001 to 6,000,000 rows
    #   pause:
    #     seconds: 40
    # - command: /usr/bin/psql -c "INSERT INTO t1 VALUES (generate_series(5500001,6000000), now(), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text));"
    #   become: true
    #   become_user: "{{ postgresql_user }}"
    #
    # - name: KAFKA PRODUCER OVERLAY | pausing before inserting from 6,000,001 to 6,500,000 rows
    #   pause:
    #     seconds: 40
    # - command: /usr/bin/psql -c "INSERT INTO t1 VALUES (generate_series(6000001,6500000), now(), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text));"
    #   become: true
    #   become_user: "{{ postgresql_user }}"
    #
    # - name: KAFKA PRODUCER OVERLAY | pausing before inserting from 6,500,001 to 7,000,000 rows
    #   pause:
    #     seconds: 40
    # - command: /usr/bin/psql -c "INSERT INTO t1 VALUES (generate_series(6500001,7000000), now(), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text));"
    #   become: true
    #   become_user: "{{ postgresql_user }}"
    #
    # - name: KAFKA PRODUCER OVERLAY | pausing before inserting from 7,000,001 to 7,500,000 rows
    #   pause:
    #     seconds: 40
    # - command: /usr/bin/psql -c "INSERT INTO t1 VALUES (generate_series(7000001,7500000), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text));"
    #   become: true
    #   become_user: "{{ postgresql_user }}"
    #
    # - name: KAFKA PRODUCER OVERLAY | pausing before inserting from 7,500,001 to 8,000,000 rows
    #   pause:
    #     seconds: 40
    # - command: /usr/bin/psql -c "INSERT INTO t1 VALUES (generate_series(7500001,8000000), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text));"
    #   become: true
    #   become_user: "{{ postgresql_user }}"
    #
    # - name: KAFKA PRODUCER OVERLAY | pausing before inserting from 8,000,001 to 8,500,000 rows
    #   pause:
    #     seconds: 40
    # - command: /usr/bin/psql -c "INSERT INTO t1 VALUES (generate_series(8000001,8500000), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text));"
    #   become: true
    #   become_user: "{{ postgresql_user }}"
    #
    # - name: KAFKA PRODUCER OVERLAY | pausing before inserting from 8,500,001 to 9,000,000 rows
    #   pause:
    #     seconds: 40
    # - command: /usr/bin/psql -c "INSERT INTO t1 VALUES (generate_series(8500001,9000000), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text));"
    #   become: true
    #   become_user: "{{ postgresql_user }}"
    #
    # - name: KAFKA PRODUCER OVERLAY | pausing before inserting from 9,000,001 to 9,500,000 rows
    #   pause:
    #     seconds: 40
    # - command: /usr/bin/psql -c "INSERT INTO t1 VALUES (generate_series(9000001,9500000), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text));"
    #   become: true
    #   become_user: "{{ postgresql_user }}"
    #
    # - name: KAFKA PRODUCER OVERLAY | pausing before inserting from 9,500,001 to 10,000,000 rows
    #   pause:
    #     seconds: 40
    # - command: /usr/bin/psql -c "INSERT INTO t1 VALUES (generate_series(9500001,10000000), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text));"
    #   become: true
    #   become_user: "{{ postgresql_user }}"
    #
    # - name: KAFKA PRODUCER OVERLAY | pausing before inserting from 10,000,001 to 10,500,000 rows
    #   pause:
    #     seconds: 40
    # - command: /usr/bin/psql -c "INSERT INTO t1 VALUES (generate_series(10000001,10500000), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text));"
    #   become: true
    #   become_user: "{{ postgresql_user }}"
    #
    # - name: KAFKA PRODUCER OVERLAY | pausing before inserting from 10,500,001 to 11,000,000 rows
    #   pause:
    #     seconds: 40
    # - command: /usr/bin/psql -c "INSERT INTO t1 VALUES (generate_series(10500001,11000000), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text));"
    #   become: true
    #   become_user: "{{ postgresql_user }}"
    #
    # - name: KAFKA PRODUCER OVERLAY | pausing before inserting from 11,000,001 to 11,500,000 rows
    #   pause:
    #     seconds: 40
    # - command: /usr/bin/psql -c "INSERT INTO t1 VALUES (generate_series(11000001,11500000), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text));"
    #   become: true
    #   become_user: "{{ postgresql_user }}"
    #
    # - name: KAFKA PRODUCER OVERLAY | pausing before inserting from 11,500,001 to 12,000,000 rows
    #   pause:
    #     seconds: 40
    # - command: /usr/bin/psql -c "INSERT INTO t1 VALUES (generate_series(11500001,12000000), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text));"
    #   become: true
    #   become_user: "{{ postgresql_user }}"
    #
    # - name: KAFKA PRODUCER OVERLAY | pausing before inserting from 12,000,001 to 12,500,000 rows
    #   pause:
    #     seconds: 40
    # - command: /usr/bin/psql -c "INSERT INTO t1 VALUES (generate_series(12000001,12500000), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text));"
    #   become: true
    #   become_user: "{{ postgresql_user }}"
    #
    # - name: KAFKA PRODUCER OVERLAY | pausing before inserting from 12,500,001 to 13,000,000 rows
    #   pause:
    #     seconds: 40
    # - command: /usr/bin/psql -c "INSERT INTO t1 VALUES (generate_series(12500001,13000000), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text));"
    #   become: true
    #   become_user: "{{ postgresql_user }}"
    #
    # - name: KAFKA PRODUCER OVERLAY | pausing before inserting from 13,000,001 to 13,500,000 rows
    #   pause:
    #     seconds: 40
    # - command: /usr/bin/psql -c "INSERT INTO t1 VALUES (generate_series(13000001,13500000), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text));"
    #   become: true
    #   become_user: "{{ postgresql_user }}"
    #
    # - name: KAFKA PRODUCER OVERLAY | pausing before inserting from 13,500,000 to 14,000,000
    #   pause:
    #     seconds: 40
    # - command: /usr/bin/psql -c "INSERT INTO t1 VALUES (generate_series(13500001,14000000), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text));"
    #   become: true
    #   become_user: "{{ postgresql_user }}"
    #
    # - name: KAFKA PRODUCER OVERLAY | pausing before inserting from 14,500,000 to 14,500,000
    #   pause:
    #     seconds: 40
    # - command: /usr/bin/psql -c "INSERT INTO t1 VALUES (generate_series(14000001,14500000), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text));"
    #   become: true
    #   become_user: "{{ postgresql_user }}"
    #
    # - name: KAFKA PRODUCER OVERLAY | pausing before inserting from 14,500,000 to 15,000,000
    #   pause:
    #     seconds: 40
    # - command: /usr/bin/psql -c "INSERT INTO t1 VALUES (generate_series(14500001,15000000), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text));"
    #   become: true
    #   become_user: "{{ postgresql_user }}"